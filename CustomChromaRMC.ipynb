{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we have defined a custom DSpy RetirverModelClient\n",
    "\n",
    "The client uses OllamaEmbeddingFunction to  fetch  answers.\n",
    "\n",
    "The OllamaEmbeddingFunction is also provided Here\n",
    "\n",
    "\n",
    "This OllamaEmbeddingFunction  uses Ollama python library to build the interface between ollama embedder and retriever\n",
    "with a collection that is created by initialize_chromadb_collection method\n",
    "\n",
    "The initialize_chromadb_collection method depends on OllamaEmbeddingFunction for writing vectors to the chromadb server\n",
    "\n",
    "DSPythonicRMClient uses OllamaEmbeddingFunction for reading passages from chromadb server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OllamaEmbeddingFunction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n",
    "from typing import Optional, Union, List\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class OllamaEmbeddingFunction(EmbeddingFunction[Documents]):\n",
    "    Documents = Union[str, List[str], pd.DataFrame]\n",
    "\n",
    "    def __init__(self, model_name: str = \"mxbai-embed-large\", collection=None):\n",
    "        \"\"\"Initialize the embedding function.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.collection = collection  # Store the collection\n",
    "\n",
    "    def __call__(self, input: Documents) -> List[List[float]]:\n",
    "        \"\"\"Embed the input documents.\"\"\"\n",
    "        return self._embed(input)\n",
    "\n",
    "    def _embed(self, documents: Documents) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for the input documents using Ollama.\"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        # Handle different input types\n",
    "        if isinstance(documents, str):\n",
    "            # If input is a single string, convert it to a list\n",
    "            documents = [documents]\n",
    "        elif isinstance(documents, pd.DataFrame):\n",
    "            # If input is a DataFrame, extract the first column as a list of strings\n",
    "            documents = documents.iloc[:, 0].tolist()\n",
    "        elif isinstance(documents, list):\n",
    "            # If input is a list, ensure all elements are strings\n",
    "            documents = [str(doc) for doc in documents]\n",
    "        elif hasattr(documents, '__iter__'):\n",
    "            # If input is any other iterable (like a set or tuple), convert to list of strings\n",
    "            documents = [str(doc) for doc in documents]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported document type. Please provide a string, list, or pandas DataFrame.\")\n",
    "\n",
    "        # Generate embeddings for each document\n",
    "        for doc in documents:\n",
    "            response = ollama.embeddings(\n",
    "                model=self.model_name,\n",
    "                prompt=doc\n",
    "            )\n",
    "            embeddings.append(response[\"embedding\"])\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def _retrieve(self, query: Union[str, List[str]], n_results: int) -> List[str]:\n",
    "        \"\"\"Retrieve relevant documents based on a query using Ollama.\"\"\"\n",
    "        if self.collection is None:\n",
    "            raise ValueError(\"Collection is not set. Please initialize the OllamaEmbeddingFunction with a valid collection.\")\n",
    "\n",
    "        # Handle different input types for query\n",
    "        if isinstance(query, list):\n",
    "            # If input is a list, join it into a single string\n",
    "            query = ' '.join(query)\n",
    "\n",
    "        response = ollama.embeddings(\n",
    "            model=self.model_name,\n",
    "            prompt=query\n",
    "        )\n",
    "        query_embedding = response[\"embedding\"]\n",
    "\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results\n",
    "        )\n",
    "\n",
    "        return results['documents'][:n_results]  # Return the top n_results documents in correct order\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Robust ChromaDB Collection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import chromadb\n",
    "from typing import Optional\n",
    "import random\n",
    "import chromadb\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# Create a dictionary to store the last used collection name and serial number\n",
    "global last_used_info\n",
    "\n",
    "last_used_info = {}\n",
    "\n",
    "\n",
    "\n",
    "def initialize_chromadb_collection(host: str = 'localhost', port: int = 8000, reset: Optional[bool] = False, create_new_collection: bool = True, last_used: Optional[dict] = None) -> chromadb.Collection:\n",
    "    \"\"\"\n",
    "    Initializes a ChromaDB HTTP client and creates or retrieves a collection.\n",
    "\n",
    "    Args:\n",
    "        host (str): The host where the ChromaDB server is running. Defaults to 'localhost'.\n",
    "        port (int): The port on which the ChromaDB server is listening. Defaults to 8000.\n",
    "        reset (Optional[bool]): If True, resets the ChromaDB client before creating or using a collection. Defaults to False.\n",
    "        create_new_collection (bool): If True, creates a new collection with a serial numbered name. If False, uses the last used collection name. Defaults to True.\n",
    "        last_used (Optional[dict]): A dictionary to store the last used collection name and number. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        chromadb.Collection: The created or existing ChromaDB collection.\n",
    "    \"\"\"\n",
    "    # Initialize last_used if it is None\n",
    "    if last_used is None:\n",
    "        last_used = {'collection_name': None, 'serial_number': 0}\n",
    "    elif 'serial_number' not in last_used:\n",
    "        last_used['serial_number'] = 0\n",
    "\n",
    "    # Create a ChromaDB HTTP client\n",
    "    client = chromadb.HttpClient(host=host, port=port)\n",
    "    \n",
    "    # Reset the client if requested\n",
    "    if reset:\n",
    "        client.reset()\n",
    "    \n",
    "    if create_new_collection:\n",
    "        # Increment the serial number for the new collection name\n",
    "        last_used['serial_number'] += 1\n",
    "        collection_name = f\"docs{last_used['serial_number']}\"\n",
    "        \n",
    "        # Use get_or_create_collection to avoid UniqueConstraintError\n",
    "        collection = client.get_or_create_collection(name=collection_name)\n",
    "        \n",
    "        # Store the collection name for future use\n",
    "        last_used['collection_name'] = collection_name\n",
    "    else:\n",
    "        # Use the last used collection name\n",
    "        collection_name = last_used.get('collection_name')\n",
    "        \n",
    "        if collection_name is None:\n",
    "            raise ValueError(\"No previous collection name found. Set create_new_collection to True to create a new collection.\")\n",
    "        \n",
    "        # Get or create the collection with the last used name\n",
    "        collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    # Print the name of the created or used collection\n",
    "    print(f\"Using collection: {collection.name}\")\n",
    "    \n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now comes the RMC\n",
    "\n",
    "This RMC cannot (currently) fetch from any url / port and it is dependent on OllamaEmbeddingFunction  for encoding decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Optional\n",
    "import dspy\n",
    "\n",
    "\n",
    "class DSPythonicRMClient(dspy.Retrieve):\n",
    "    def __init__(self, embedding_function: OllamaEmbeddingFunction, k: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the DSPythonicRMClient.\n",
    "\n",
    "        Args:\n",
    "            embedding_function (OllamaEmbeddingFunction): The embedding function to use for retrieval.\n",
    "            k (int): The number of top passages to retrieve. Defaults to 3.\n",
    "        \"\"\"\n",
    "        super().__init__(k=k)\n",
    "        self.embedding_function = embedding_function\n",
    "\n",
    "    def forward(self, query: Union[str, List[str]], n_results: Optional[int] = None) -> dspy.Prediction:\n",
    "        \"\"\"\n",
    "        Retrieve passages based on the embedded query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string for which to retrieve passages.\n",
    "            n_results (Optional[int]): The number of results to return. Defaults to k.\n",
    "\n",
    "        Returns:\n",
    "            dspy.Prediction: An object containing the retrieved passages.\n",
    "        \"\"\"\n",
    "        n_results = n_results if n_results is not None else self.k\n",
    "        retrieved_documents = self.embedding_function._retrieve(query, n_results=n_results)\n",
    "\n",
    "        return dspy.Prediction(passages=retrieved_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use With Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using collection: docs1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Make sure to download the punkt tokenizer if you haven't already\n",
    "\"\"\" nltk.download('punkt') \"\"\"\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"Recursively searches for .md, .docx, and .txt files in the given folder path and its subfolders.\"\"\"\n",
    "    documents = []\n",
    "    data = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            if filename.endswith('.md'):\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "            \n",
    "\n",
    "            elif filename.endswith('.txt'):\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "            else:\n",
    "                continue  # Skip files that are not .md or .txt\n",
    "\n",
    "            # Split the content into sentences\n",
    "            sentences = sent_tokenize(content)\n",
    "\n",
    "            documents.extend(sentences)  # Add sentences to the documents list\n",
    "            data.append({'index': len(documents) - len(sentences), 'filename': filename, 'content': sentences})\n",
    "            #print(pd.DataFrame(data).to_markdown)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Initialize ChromaDB Collection\n",
    "collection = initialize_chromadb_collection(create_new_collection=True) #, last_used=last_used_info\n",
    "\n",
    "# Step 2: Create an instance of OllamaEmbeddingFunction\n",
    "embedding_function = OllamaEmbeddingFunction(model_name=\"mxbai-embed-large\", collection=collection)\n",
    "\n",
    "# Step 3: Embed Documents and Add Them to the Collection\n",
    "documents = load_documents(\"/home/riju279/Documents/writings/Obsidian/Videodraft2/\") #str(input(\"Enter the absoluter path to the folder\"))\n",
    "last_used_info = {}\n",
    "# Embed documents and add them to the collection\n",
    "embeddings = embedding_function(documents)\n",
    "collection.add(\n",
    "    ids=[str(i) for i in range(1, len(documents) + 1)],\n",
    "    embeddings=embeddings,\n",
    "    documents=documents\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above code is a fully working example for making embeddings out of markdown files from a given Directory\n",
    "\n",
    "getting proper infrastructure for doing large scale dspy experiments was important for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create an instance of DSPythonicRMClient and LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lrm = DSPythonicRMClient(embedding_function=embedding_function, k=10)\n",
    "olm=dspy.OpenAI(api_base=\"http://localhost:11434/v1/\", api_key=\"ollama\", model=\"mistral-nemo:latest\", stop='\\n\\n', model_type='chat')\n",
    "\n",
    "dspy.settings.configure(lm=olm,rm=lrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "class GenerateQuestion(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.OutputField(desc=\"ask questions about the facts\")\n",
    "    \n",
    "\n",
    "\n",
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_question = dspy.ChainOfThought(GenerateQuestion)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_question(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
    "    \n",
    "import time\n",
    "\n",
    "generate_question = dspy.ChainOfThoughtWithHint(GenerateQuestion)\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    context=documents[i]\n",
    "    hint= f\"ask why, how and what about {documents[i]}\"\n",
    "    pred=generate_question(context=context, hint=hint)\n",
    "    print(f\"{i}\\n Context: {context}\\n\\n\")\n",
    "    print(f\"Predicted Question: {pred.question}\\n\\n -----\\n\\n######\\n\\n\")\n",
    "    time.sleep(2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
