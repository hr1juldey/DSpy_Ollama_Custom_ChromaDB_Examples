{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## go to your system terminal and type ``ollama serve`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start with a simple example where mostly everything is in the notebook\n",
    "\n",
    "here we have a chromadb one ollama server and  a story  written by yours truely [`Perplexity`](https://www.perplexity.ai/)\n",
    "\n",
    "and a set of questions also related to the story to test the rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "\"What was the main reason Lily decided to explore the hidden garden?\",\n",
    "\"How did Jack describe the flowers in the garden?\",\n",
    "\"What personal struggles did Lily and Jack share with each other?\",\n",
    "\"What did the owl say when it first appeared to Lily and Jack?\",\n",
    "\"What wish did Lily make for Jack?\",\n",
    "\"How did the flowers react when Lily and Jack danced around them?\",\n",
    "\"What did the owl mean by saying 'kindness must be shared'?\",\n",
    "\"How did the setting of the garden contribute to the story?\",\n",
    "\"What emotions did Jack express after Lily made her wish?\",\n",
    "\"How did the relationship between Lily and Jack develop throughout the story?\"\n",
    "]\n",
    "\n",
    "\n",
    "# don't run this code  as the variable is same in later code python will get confused \n",
    "# use the question manually of use your own list/ variable to call this list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below there is a story \n",
    "\n",
    "It will be sent to the **RAG** pipeline as a list of sentences toekenized by nltk \"punkt\" and \"punkt_tab\"\n",
    "there will be a basic chromadb client set up and running at localhost:8000\n",
    "there will be a  random collection created for storing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/riju279/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/riju279/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "import random \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "story=\"\"\"\n",
    "\n",
    "## The Hidden Garden.\n",
    "\n",
    "In a small town, there was a garden hidden behind a tall wall. The townsfolk often whispered about it, saying it was enchanted. No one had seen the inside, but everyone had their own stories.\n",
    "\n",
    "Some said it was filled with flowers that could sing, while others believed it was home to a wise old owl that could grant wishes. \n",
    "\n",
    "One sunny afternoon, a curious girl named Lily decided to explore the garden. \n",
    "\n",
    "Lilly had always been fascinated by the stories and wanted to see if they were true. \n",
    "\n",
    "As she approached the wall, she noticed a small, rusty gate slightly ajar. \n",
    "\n",
    "With her heart racing, she pushed it open and stepped inside.\n",
    "\n",
    "The garden was more beautiful than she had imagined. \n",
    "\n",
    "Colorful flowers bloomed everywhere, and the air was filled with sweet scents. \n",
    "\n",
    "In the center stood an ancient oak tree, its branches stretching wide, offering shade to a small stone bench. \n",
    "\n",
    "As she walked closer, she noticed a figure sitting on the bench—a boy about her age, with messy brown hair and bright green eyes.\n",
    "\n",
    "“Hi, I’m Jack,” he said, smiling. “I come here to think.”\n",
    "\n",
    "“Hi, I’m Lily,” she replied, feeling a strange connection with him. “I’ve heard so many stories about this place.”\n",
    "\n",
    "Jack nodded. “Most of them are true. The flowers do sing, but only when they feel happy. And the owl? He’s real too. He watches over the garden.”\n",
    "\n",
    "Lily’s eyes widened. “Can we see him?”\n",
    "\n",
    "Jack hesitated. “He only appears when he senses someone who truly believes in magic.”\n",
    "\n",
    "Lily felt a spark of determination. “I believe! Let’s find him!”\n",
    "\n",
    "Together, they explored the garden, laughing and sharing stories. They discovered a patch of flowers that began to hum softly when they danced around them. The more they danced, the louder the flowers sang, filling the air with joy.\n",
    "\n",
    "As they played, they talked about their lives. Lily shared how she often felt lonely at school, while Jack revealed that he had a sick sister who couldn’t leave their home. They both understood what it felt like to wish for something more.\n",
    "\n",
    "Suddenly, the air grew still, and a soft hoot echoed through the garden. They turned to see a majestic owl perched on a low branch of the oak tree. Its feathers shimmered in the sunlight, and its eyes sparkled with wisdom.\n",
    "\n",
    "“Welcome, young ones,” the owl said in a deep, soothing voice. “You have shown kindness and belief. What is it that you seek?”\n",
    "\n",
    "Lily and Jack exchanged glances, unsure of what to say. Finally, Lily spoke up. “I wish for Jack’s sister to get better.”\n",
    "\n",
    "The owl nodded slowly. “True friendship and selflessness are the greatest forms of magic. I will grant your wish, but remember, kindness must be shared.”\n",
    "\n",
    "With a flap of its wings, the owl vanished into the air, leaving behind a soft glow. Jack looked at Lily, tears of gratitude in his eyes. “Thank you, Lily. You’ve given me hope.”\n",
    "\n",
    "As the sun began to set, they promised to meet again in the garden. They had found not only magic but also a friendship that would last a lifetime.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#\n",
    "documents = sent_tokenize(story)\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = documents[i].replace('\\n', '')\n",
    "\n",
    "#print(documents)\n",
    "j=random.randint(0,29999)\n",
    "\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = client.create_collection(name=f\"docs{j}\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=d)\n",
    "  embedding = response[\"embedding\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=[embedding],\n",
    "    documents=[d]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will do the real RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "\n",
      " The Retrieved data is: \n",
      "\n",
      " ['He watches over the garden.”Lily’s eyes widened.', 'Let’s find him!”Together, they explored the garden, laughing and sharing stories.', 'The garden was more beautiful than she had imagined.', 'Some said it was filled with flowers that could sing, while others believed it was home to a wise old owl that could grant wishes.', 'Jack looked at Lily, tears of gratitude in his eyes.', 'They discovered a patch of flowers that began to hum softly when they danced around them.', '“I’ve heard so many stories about this place.”Jack nodded.', '## The Hidden Garden.', 'Colorful flowers bloomed everywhere, and the air was filled with sweet scents.'] \n",
      "\n",
      "\n",
      "--------\n",
      "The Answer is:\n",
      "\n",
      " ------------\n",
      "\n",
      " Jack didn't directly describe the flowers in his dialogue or thoughts within the provided data. However, it is mentioned that \"Colorful flowers bloomed everywhere\" and \"Some said it was filled with flowers that could sing.\" So, based on these descriptions, Jack likely saw the flowers as colorful and musical. \n",
      "\n",
      " the answer is given by: mistral-nemo:latest\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# an example prompt\n",
    "\n",
    "prompt = \"How did Jack describe the flowers in the garden?\"\n",
    "n_results=10\n",
    "# generate an embedding for the prompt and retrieve the most relevant doc\n",
    "response = ollama.embeddings(\n",
    "  prompt=prompt,\n",
    "  model=\"mxbai-embed-large\"\n",
    ")\n",
    "results = collection.query(\n",
    "  query_embeddings=[response[\"embedding\"]],\n",
    "  n_results=n_results\n",
    ")\n",
    "print(\"-------\\n\\n The Retrieved data is: \\n\\n\",results['documents'][0][:int(n_results-1)],\"\\n\\n\\n--------\")\n",
    "\n",
    "data = [doc[0:] for doc in results['documents'][:int(n_results-1)]]\n",
    "\n",
    "# Now 'data' contains the 3 most relevant documents\n",
    "#print(f\"context is \\n\\n {data} \\n\\n\")\n",
    "\n",
    "# generate a response combining the prompt and data we retrieved in step 2\n",
    "LM=\"mistral-nemo:latest\"\n",
    "\n",
    "output = ollama.generate(\n",
    "  model=LM,\n",
    "  prompt=f\"Using this data: {data}. Respond to this prompt: {prompt}\"\n",
    ")\n",
    "\n",
    "print(\"The Answer is:\\n\\n ------------\\n\\n\",output['response'],f\"\\n\\n the answer is given by: {LM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whatever we saw here, was the basic lag Pipeline without any involvement of DSPY\n",
    "\n",
    "now comes the magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working example of a simple **RAG** where embedding and retrieval is managed by ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a dump file name it `data.md`,   in the same directory you are running this code.\n",
    "else use an absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " The answer is:    Beautifully singing \n",
      "\n",
      " because: produce the answer. We need to find where Jack speaks about the flowers in the garden. The context mentions \"flowers that could sing,\" but it doesn't specify who said this. However, since we're looking for how Jack described them, and there's no direct quote from him, we can infer based on the overall description of the garden \n",
      "\n",
      " The long form context is [['He watches over the garden.”\\n\\nLily’s eyes widened.', 'Let’s find him!”\\n\\nTogether, they explored the garden, laughing and sharing stories.', 'The garden was more beautiful than she had imagined.', 'Some said it was filled with flowers that could sing, while others believed it was home to a wise old owl that could grant wishes.', 'Jack looked at Lily, tears of gratitude in his eyes.']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import dspy\n",
    "import ollama\n",
    "import chromadb\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "### Press Ctrl + Shift + A after selecting the bottom section to get those modules\n",
    "\"\"\" nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\") \"\"\"\n",
    "\n",
    "# Initialize a random collection name\n",
    "j = random.randint(0, 29999)\n",
    "\n",
    "# Create a ChromaDB client\n",
    "client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "collection = client.create_collection(name=f\"docs{j}\")\n",
    "\n",
    "# Define the path to the Markdown file\n",
    "markdown_file_path = str(input(\"Please enter the absolute path to your markdown file\"))  # Update this path to your Markdown file\n",
    "\n",
    "# Read the Markdown file and store its content in a string variable named 'file'\n",
    "with open(markdown_file_path, 'r', encoding='utf-8') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "\n",
    "# Store documents in the vector embedding database\n",
    "documents = sent_tokenize(file_content)  # Replace with your actual documents\n",
    "\n",
    "# made an embedding function\n",
    "def embedOllama(texts):\n",
    "    for i, d in enumerate(texts):\n",
    "        response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=d)\n",
    "        embedding = response[\"embedding\"]\n",
    "        collection.add(\n",
    "        ids=[str(i)],\n",
    "        embeddings=[embedding],\n",
    "        documents=[d]\n",
    "    )\n",
    "\n",
    "# called embedOllama on documents\n",
    "embedOllama(documents)\n",
    "\n",
    "# Function to retrieve relevant documents based on a query\n",
    "def retrieve_documents(prompt, n_results=10):\n",
    "    # Generate an embedding for the prompt\n",
    "    response = ollama.embeddings(\n",
    "        prompt=prompt,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "    \n",
    "    # Query the collection for the most relevant documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[response[\"embedding\"]],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    # Print the top results\n",
    "    #print(results['documents'][0][:int(n_results)], \"\\n\\n\\n--------\")\n",
    "    \n",
    "    # Prepare data for further processing\n",
    "    data = [doc[0:] for doc in results['documents'][:int(n_results-1)]]\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "\"\"\" prompt = \"describe AI in Enhanced Customer Service\"\n",
    "retrieved_docs = retrieve_documents(prompt,2) \"\"\"\n",
    "\n",
    "#######################################################\n",
    "#        #### working code for retirval ####          #\n",
    "#######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "olm=dspy.OpenAI(api_base=\"http://localhost:11434/v1/\", api_key=\"ollama\", model=\"mistral-nemo:latest\", stop='\\n\\n', model_type='chat')\n",
    "\n",
    "\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions with detailed factoid answers.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 5 and 10 words\")\n",
    "\n",
    "\n",
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Write a simple search query that will help answer a complex question.\"\"\"\n",
    "\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    query = dspy.OutputField()\n",
    "\n",
    "\n",
    "generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "#generate_answer=dspy.ChainOfThought(GenerateSearchQuery)\n",
    "\n",
    "# Call the predictor on a particular input alongside a hint.\n",
    "\n",
    "\n",
    "question='How did Jack describe the flowers in the garden?'\n",
    "# pre query db for context\n",
    "context=retrieve_documents(question,5)\n",
    "# send question and context to db\n",
    "\n",
    "prompt=str(f\"tell me why{[question,context]}\")\n",
    "# post query db for better answer\n",
    "\n",
    "\n",
    "retrieved_docs = retrieve_documents(question,n_results=3) #  (GenerateSearchQuery) ## changed\n",
    "dspy.configure(lm=olm,rm=retrieved_docs)\n",
    "\n",
    "\n",
    "pred = generate_answer(question=question,context=context)\n",
    "\n",
    "print(f\"\\n\\n\\n The answer is:    {pred['answer']} \\n\\n because: {pred['rationale']} \\n\\n The long form context is {context}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
